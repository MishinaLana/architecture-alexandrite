#Мотивация

Текущие системные проблемы, такие как потеря контрактов, недовольство клиентов, внутренние задержки не могут быть диагностированы с помощью Яндекс.Метрика. 
Требуется внедрение комплексного мониторинга платформы для решения задач:

- Проактивное обнаружение инцидентов до воздействия на клиентов: увеличение времени ответа на запросы, накопление очередей.

- Определение коренных причин сбоев (БД или фронтенд)

- Обоснование масштабирования инфраструктуры на основе объективных метрик (CPU, память)

- Обеспечение прозрачности работы системы

- Сокращение времени релизов за счет автоматизированного тестирования стабильности



#Выбор подхода к мониторингу

Для различных компонентов системы применяются свои подходы мониторинга

1. Мониторинг бизнес-логики (онлайн-магазин, CRM, MES API)

Метод: Четыре золотых сигнала

Обоснование: Оценка работы сервисов с точки зрения пользовательского опыта

2. Мониторинг инфраструктуры (БД, очереди, серверы)

Метод: USE

Обоснование: Выявление узких мест инфраструктуры, влияющих на работу сервисов

3. Мониторинг асинхронных процессов (RabbitMQ)

Метод: USE 

Обоснование: Контроль целостности message-driven архитектуры, где потеря сообщения критична для бизнеса



Метрики

1. Метрики для API:

- Number of requests (RPS) for internet shop API
- Number of requests (RPS) for CRM API
- Number of requests (RPS) for MES API
  
Покажут нагрузку на каждый сервис. Помогут выявить аномальные всплески или падения трафика
Ярлыки: service, endpoint

- Response time (latency) for shop API
- Response time (latency) for CRM API	
- Response time (latency) for MES API	

Позволит найти медленные эндпоинты
Ярлыки: service, endpoint, quantile

- Number of HTTP 200/500 for shop API
- Number of HTTP 200/500 for CRM API
- Number of HTTP 200/500 for MES API

Оценить уровень ошибок
Ярлыки: service, endpoint, http_status

- Number of requests (RPS) per user for MES API	

Выявить слишком активных API-партнёров, которые создают избыточную нагрузку
Ярлыки: service, user_id

2. Метрики для инфраструктуры:

- CPU Utilization for shop API
- CPU Utilization for CRM API
- CPU Utilization for MES API

Показывает загрузку вычислительных ресурсов. Высокий CPU может быть причиной высокой латенции
Ярлыки: service, instance_id

- Memory Utilisation for shop API
- Memory Utilisation for CRM API
- Memory Utilisation for MES API

Помогает выявить утечки памяти, которые могут приводить к перезапускам сервисов
Ярлыки: service, instance_id

- Kb provided (sent) for MES API

Большой размер отправляемых данных (3D-модели) напрямую влияет на сетевую нагрузку и время ответа
Ярлыки: service, endpoint

3. Метрики для Базы данных:

- Number of connections for shop db instance
- Number of connections for MES db instance

Приближение к лимиту подключений БД может быть причиной ошибок 5xx и медленной работы API
Ярлыки: db_instance

- Memory Utilisation for shop db instance
- Memory Utilisation for MES db instance

Нехватка памяти у БД приводит к постоянному чтению с диска и резкому падению производительности
Ярлыки: db_instance

- Size of shop db instance
- Size of MES db instance

Контроль за ростом данных. Помогает планировать масштабирование хранилища
Ярлыки: db_instance

4. Метрики для RabbitMQ:

- Number of message in flight in RabbitMQ

Растущее число сообщений указывает, что потребитель (CRM, MES) не успевает их обрабатывать, что является прямой причиной задержек в обновлении статусов заказов
Ярлыки: queue_name (например, orders.to_crm)

- Number of dead-letter-exchange letters in RabbitMQ

Наличие сообщений в DLQ означает, что они не были обработаны корректно и требуют внимания разработчиков
Ярлыки: queue_name, reason



#План действий

1. Развертывание стека мониторинга

Prometheus: Установка и настройка как time-series базы данных
Grafana: Развертывание для визуализации и дашбордов
Alertmanager: Настройка системы оповещений

2. Инструментирование приложений

Java-сервисы (Магазин, CRM): Интеграция Micrometer
C#-сервис (MES): Внедрение prometheus-net
Frontend (Vue/React): Настройка Grafana Faro для отслеживания пользовательских действий

3. Настройка экспортеров инфраструктуры

node_exporter: На всех EC2-инстансах для метрик ОС
postgres_exporter: Для мониторинга PostgreSQL
rabbitmq_exporter: Для сбора метрик RabbitMQ

4. Создание ключевых дашбордов

Дашборд "4 золотых сигнала" для каждого сервиса
Дашборд "Инфраструктура" с метриками БД, очередей и инстансов
Дашборд "Бизнес-статусы заказов" с аналитикой времени обработки

5. Настройка системы оповещений

Определение пороговых значений для критических метрик
Интеграция с Slack/Telegram для уведомлений команды
Настройка создания тикетов в Jira для критических инцидентов

6. Интеграция в процесс разработки

Настройка экспорта метрик из dev и release окружений
Включение проверки метрик в критерии QA перед релизом:
Отсутствие роста 500-х ошибок
Соответствие latency требуемым значениям
Стабильность ключевых бизнес-метрик


Приоритет выполнения:

Развертывание базового стека (Prometheus/Grafana)
Инструментирование критических сервисов (MES, CRM)
Настройка экспортеров инфраструктуры
Создание дашбордов для ключевых метрик
Настройка системы алертинга
Интеграция в процесс разработки



#Дополнительное задание

1. Высокая задержка MES API

Метрика: MES API Latency (p95) > 30s
Порог: 30 секунд
Воздействие: Риск отказа пользователей от заказов
Реакция: Автоматическое горизонтальное масштабирование. Оповещение в Slack-канал разработчиков. Запуск задачи по оптимизации алгоритма расчета

2. Накопление сообщений в очереди

Метрика: Number of messages in flight > 100 в очереди orders.to_crm
Порог: 100 сообщений
Воздействие: CRM не справляется с обработкой заказов
Реакция: Оповещение в Slack-каналы команды и DevOps. Создание тикета с приоритетом High. Проверка состояния CRM-сервиса и базы данных

3. Высокая загрузка CPU

Метрика: CPU Utilization > 80% в течение 5 минут (инстанс MES API)
Порог: 80%
Воздействие: Риск роста задержек и ошибок
Реакция: Автоматическое горизонтальное масштабирование

4. Превышение лимита подключений к БД

Метрика: Database Connections > 90% от лимита
Порог: 90%
Воздействие: Риск ошибок "Too many connections"
Реакция: Оповещение в Slack-каналы DevOps и разработчиков БД. Создание тикета на увеличение лимита/оптимизацию запросов. Рассмотрение шардирования БД

5. Критическое количество ошибок сервера

Метрика: HTTP 500 Errors > 10 в минуту для любого API
Порог: 10 ошибок/минуту
Воздействие: Активная деградация сервиса
Реакция: Оповещение через PagerDuty/Slack. Автоматическое создание инцидента в Jira. Немедленное начало расследования причины
